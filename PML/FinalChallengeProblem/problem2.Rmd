---
title: "problem2"
author: "Joseph Walker"
date: "3/26/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = F, message = F)
library(tidyverse)
library(caret)
```

# Problem 2

**Use the dataset ocdata.csv having the following fields - education, income, women, prestige, census, type - to answer the questions below.**

**(Problem 2A) Fit a univariate OLSR (Ordinary Least Squares Regression) model, adhering to OLSR assumptions, predicting income from prestige only.**
  
**(Problem 2B) Fit a model of any type we discussed in class, using all meaningful predictors of income, to obtain the "best" results, using whatever method you wish. Use of caret for Problem 2 may be helpful, but is not required. Be sure to properly prepare your data, take steps to avoid overfitting, tune and evaluate the performance of your model, and provide a clear description and analysis of your results. Caution: for your solution, more is not better. Use only an appropriate model and methods that provides value. Avoid unnecessary work. Also, pay attention to the appearance and quality of your HTML product: attractive, crisp, and clear.**

First we'll begin by importing the data set.

```{r}
#read in the dataset
ocdata <- read_csv("ocdata.csv")

#examine the dataset
glimpse(ocdata)
```

The only variable that is not numeric is `type`. Let's take a closer look at this variable to see if we need to convert it to a categorical variable (factor).

```{r}
#examine the distinct values in the type variable
ocdata %>%
  select(type) %>%
  distinct()

#make the type variable a factor
ocdata$type <- factor(ocdata$type)
```

Now let's summarize the data to look for anything that might stick out immediately.

```{r}
summary(ocdata)
```

There are four NA's in the *type* variable. We'll have to filter these out so that we don't have any problems with out modeling later on.

```{r}
#omit rows with NAs
ocdata <- na.omit(ocdata)

#Check to see if there are any NA values
any(is.na(ocdata))
```


## OLSR 

### Problem 2A
Our first task is to build an ordinary least squares regression model predicting income using only the prestige variable. Before we begin, let's first visualize the relationship between the two variables to make sure there's a linear relationship.

```{r}
ggplot(ocdata, aes(x = prestige, y = income)) +
  geom_point() +
  geom_smooth(method = "lm")
```

Aha! Good thing we checked this basic but necessary assumption. It looks like there's something going on at the extreme end of the dataset where income increases much more dramatically at the highest levels of prestige.

If we were to build a model on this data as is, we'd expect the residuals to be skewed. The second assumption of OLS states that the mean of the residuals should be 0. This would indicate that the error is random and now bias still exists in the model.

Let's check to see if this is the case. First let's split the data.

```{r}
set.seed(6436)

nrows <- nrow(ocdata)

index <- sample.int(n = length(ocdata$income), size = nrows * 0.8, replace = F)

octrain <- ocdata[index, ]
octest <- ocdata[-index, ]
```

Now let's build the model.
```{r}
set.seed(47)
library(modelr)
library(broom)

#create linear model
ols <- lm(formula = income ~ prestige, data = octrain)

#examine the model results
glance(ols)

#augment the dataset using the ols model
ols_augmented <- augment(ols)

#plot the residuals
ggplot(data= ols_augmented, aes(x = .fitted, y = .resid)) +
  geom_point()

#determine the mean of the resids
mean(ols_augmented$.resid)
```

Even though the residuals practically have a mean of 0, there's clearly something going on as we see that the resids start to increase drasitcally as the predicted value increases.


To fix this, we can transform the data using a log transformation. Let's take a look.
```{r}
ggplot(ocdata, aes(x = prestige, y = log(income))) + 
  geom_point() +
  geom_smooth(method = "lm")
```

Now, let's build a model reflecting this transformation.

```{r}
set.seed(146)

#Build a log transformed OLS model
log_trans_ols <- lm(log(income) ~ prestige, data = octrain)

#exmaine the log transformed model
glance(log_trans_ols)
```

Let's look more closely at the results.

```{r}
#augment the log transformed model
log_trans_augmented <- augment(log_trans_ols)

#plot the residuals
ggplot(data= log_trans_augmented, aes(x = .fitted, y = .resid)) +
  geom_point() +
  geom_abline(slope = 0)

#plot the predicted values vs. the fitted values (transformed back into the original scale)
ggplot(log_trans_augmented, aes(x = exp(1)^.fitted, y = exp(1)^log.income.))+
  geom_point() +
  geom_abline()
```

Now, let's try make predictions on the test set and compare them to the actuals.
```{r}
#make predictions on the test set
oc_ols_test <- augment(log_trans_ols, newdata = octest)

#plot the data (don't forget to transform the predicted values back to the original scale)
ggplot(oc_ols_test, aes(x = prestige)) +
  geom_point(aes(y = income, color = "blue")) +
  geom_point(aes(y = exp(1)^.fitted, color = "red")) +
  scale_color_manual(name = "values", values = c("blue" = "blue", "red" = "red"), labels = c("actual", "predicted"))
  
```

## Problem 2B

In this next section, we'll explore the rest of the variables in the ocdata set and build a model using **glmnet**.

First, we'll create a train control for 10 fold cross validation.
```{r}
#create train control
oc_control <- trainControl(method = "cv", number = 10, verboseIter = T)
```

Next let's see if there are any model parameters we can specify for the glmnet function.

```{r}
modelLookup("glmnet")
```

The glmnet model allows us to tune alpha and lambda. pure alpha specifies a lasso regression model which penalizes non-zero coefficients. Because we only have a 4 predictor variables to work with, this may not be the best type of regression. On the other hand, we have ridge regression which penalizes the absolute value of the coefficients. We can specify the tune lambda to find the optimal model.

```{r}
#define tuning parameters
lambdas <- 10^seq(5, -2, by = -.1)
alpha <- 0

#create grid of custom tuning parameters
tunegrid <- expand.grid(alpha = alpha, lambda = lambdas)

#create x and y vars for formula
x <- octrain %>% select(-income) %>% data.matrix()

y <- octrain$income

#build ridge regression mode
oc_glm <- train(x = x, y = y, method = "glmnet", trControl = oc_control, tuneGrid = tunegrid)

oc_glm$results

plot(oc_glm)

oc_glm$bestTune
```

Now that we have found the optimal results, let's build the model with the best lambda.

```{r}
set.seed(947)

best_glm <- train(x, y, 
                  method = "glmnet", 
                  trControl = trainControl(method = "none"), 
                  tuneGrid = data.frame(alpha = 0, 
                                        lambda = oc_glm$bestTune$lambda))
summary(best_glm)

coef(best_glm$finalModel, s = best_glm$bestTune$lambda)
```

Let's apply our model to the test set
```{r}

newdata <- octest %>% select(-income) %>% data.matrix()

best_glm_preds <- predict(best_glm, newdata = newdata)

#convert the predictions back to the original scale
#best_glm_preds <- exp(1)^best_glm_preds

```

Calculate the $R^2$

```{r}
#calculate total sum of squares
sst <- sum((octest$income - mean(octest$income))^2)

#calculate the sum of the squared error
sse <- sum((best_glm_preds - octest$income)^2)

#calculate R^2
1 - (sse/sst)
```

visualize the results
```{r}
octest <- octest %>%
  mutate(predicted = best_glm_preds)

ggplot(octest, aes(x = predicted, y = income)) +
  geom_point()
```

```{r}
varImp(object = best_glm)
```


Quasipoisson model
```{r}
quasi_glm <- glm(income ~ ., family = 'quasipoisson', data = octrain)

octest_preds <- add_predictions(data = octest, model = quasi_glm)

octest_preds <- octest_preds %>%
  mutate(pred = exp(pred))

ggplot(octest_preds, aes(x = pred, y = income))+
  geom_point()
```

