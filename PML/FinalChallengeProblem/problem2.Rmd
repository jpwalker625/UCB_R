---
title: "problem2"
author: "Joseph Walker"
date: "3/26/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = F, message = F, error = T )
library(tidyverse)
library(caret)
```

# Problem 2

**Use the dataset ocdata.csv having the following fields - education, income, women, prestige, census, type - to answer the questions below.**

**(Problem 2A) Fit a univariate OLSR (Ordinary Least Squares Regression) model, adhering to OLSR assumptions, predicting income from prestige only.**
  
**(Problem 2B) Fit a model of any type we discussed in class, using all meaningful predictors of income, to obtain the "best" results, using whatever method you wish. Use of caret for Problem 2 may be helpful, but is not required. Be sure to properly prepare your data, take steps to avoid overfitting, tune and evaluate the performance of your model, and provide a clear description and analysis of your results. Caution: for your solution, more is not better. Use only an appropriate model and methods that provides value. Avoid unnecessary work. Also, pay attention to the appearance and quality of your HTML product: attractive, crisp, and clear.**

First we'll begin by importing the data set.

```{r}
#read in the dataset
ocdata <- read_csv("ocdata.csv")

#examine the dataset
glimpse(ocdata)
```

The only variable that is not numeric is `type`. Let's take a closer look at this variable to see if we need to convert it to a categorical variable (factor).

```{r}
#examine the distinct values in the type variable
ocdata %>%
  select(type) %>%
  distinct()

#make the type variable a factor
ocdata$type <- factor(ocdata$type)
```

Now let's summarize the data to look for anything that might stick out immediately.

```{r}
summary(ocdata)
```

There are four NA's in the *type* variable. We'll have to filter these out so that we don't have any problems with out modeling later on.

```{r}
#omit rows with NAs
ocdata <- na.omit(ocdata)

#Check to see if there are any NA values
any(is.na(ocdata))
```


## Problem 2A
Our first task is to build an ordinary least squares regression model predicting income using only the prestige variable. Before we begin, let's first visualize the relationship between the two variables to make sure there's a linear relationship.

```{r}
ggplot(ocdata, aes(x = prestige, y = income)) +
  geom_point() +
  geom_smooth(method = "lm")
```

Hmm, this is interesting. While the relationship between the two variables is linear *for the most part*, it appears that there is something happening on the right tail which results in significantly larger income values at the upper end of the prestige.

We can check to see whether or not there are any trends in the residuals after we fit the model. But first, let's split the data into a training and test set. Since our data set is rather small, I'm going to use a 70:30 train/test split rather than the traditional 80/20.

```{r}
set.seed(6436)

nrows <- nrow(ocdata)

index <- sample.int(n = length(ocdata$income), size = nrows * 0.7, replace = F)

octrain <- ocdata[index, ]
octest <- ocdata[-index, ]
```

Now let's build the model.
```{r}
set.seed(478)
library(modelr)
library(broom)

#create linear model
ols <- lm(formula = income ~ prestige, data = octrain)

#augment the dataset using the ols model
ols_augmented <- augment(ols)

#plot the residuals
ggplot(data= ols_augmented, aes(x = .fitted, y = .resid)) +
  geom_point()

#determine the mean of the resids
mean(ols_augmented$.resid)
```

The good news is that the residuals *practically* have a mean of 0. This checks off one of the necessary assumptions required of OLS regression. 

Let's see how the model performed over all. The RMSE tells us how much error is associated with each predicted value, smaller RMSE is always better. The $R^2$ tells us how well the fit of the model is. In other words, how well does the predictor variable, in this case *prestige*, explain the response variable, *income*. A value of 1 is ideal meaning that we have a perfect fit, where as 0 means we'd be just as good at guessing the values than trying to use the model to make predictions.

```{r}
sprintf("RMSE: %g", rmse(ols, octrain))
sprintf("R-squared: %g", rsquare(ols, octrain))
```

Now, let's take a look at how the model performs on the test set.

```{r}
#add predictions to the octest set
olstest_pred <- add_predictions(data = octest, model = ols, var = "predicted")

#plot actual vs. predicted
ggplot()+
  geom_point(data = olstest_pred, aes(x = predicted, y = income, color = "test data")) +
  geom_point(data= ols_augmented, aes(x = .fitted, y = income, color = "train data")) +
  scale_colour_manual(name = "Values", values = c("test data" = "red3", "train data" = "steelblue4"))
```

Both the training data and test data indicate that the model signficantly under estimates the predicted income value at the upper end, i.e. when the prestige reaches the highest levels. Let's examine this trend from another perspective.

```{r}
#plot the data
ggplot(olstest_pred, aes(x = prestige)) +
  geom_point(aes(y = income, color = "actual")) +
  geom_point(aes(y = predicted, color = "predicted")) +
  scale_color_manual(name = "Values", values = c("actual" = "red3", "predicted" = "steelblue4"))

#calculate rsquared and rmse for test set
olstest_rmse <- RMSE(pred = olstest_pred$predicted, obs = olstest_pred$income)
olstest_r.squared <- R2(pred = olstest_pred$predicted, obs = olstest_pred$income)
```

## Problem 2B

THoe model above uses only one predictor variable to generate an ordinary least squares regression model. What happens if we take a look at some of the other variables the data has to offer. And also why not try a different modeling approach. Let's start by taking a look at a scatterplot matrix of the data to see if we gain any insight.

```{r}
pairs(ocdata)
```

It appears that education shares a similar relationship to income as prestige does. To better assess this relationship we can take a look at the correlation values to see which is stronger.

```{r}
#relationship between education and income
cor(ocdata$education, ocdata$income)

#relationship between prestige and income
cor(ocdata$prestige, ocdata$income)
```

Prestige actually has the strong correlation. If we were to use both of these predictors in the dataset, we may run into the issue of overfitting the data due to co-correlation.

One interesting observation is that the income plotted agains prestige and education has a *hockey stick* trend. That is, the income values dramatically increase at the high end of the other variables producing a plot that looks like a hockey stick.

This is a frequent occurrence when dealing with monetary values. We can fix this by using a log transformation on the income response variable. Let's examine the distribution of the income data to get a better understanding of this issue.
```{r}
ggplot(ocdata, aes(x = income)) +
  geom_density()
```

We can see that there is a long right tail indicating that the income data isn't normally distributed. Most of the data is centered around 0 to 10,000 range, with a few outliers beyond that. Let's see what happens when we log transform the income.

```{r}
ggplot(ocdata, aes(x = log(income))) +
  geom_density()
```

Now the income data is normally distributed. Now let's see how we can apply this transformation to the modeling procedure.

Let's build three models, twp of which take the log transformation of the response variable income. The first model will use only prestige as the predictor, the second will include the entire dataset because we still don't know whether these other variables will provide us any additional predictive power. The third data set will not use log transformation on income and will use all available predictors to fit the model. We can think of this third model as our control to see whether taking the log of the response is better or not.

```{r}
#create formulas
log_reg_formula <- as.formula("log(income) ~ prestige")
log_multireg_formula <- as.formula("log(income) ~ .")
multi_reg_formula <- as.formula("income ~ .")

#build models
log_reg_model <- lm(log_reg_formula, octrain)
log_multi_reg_model <- lm(log_multireg_formula, octrain)
multi_reg_model <- lm(multi_reg_formula, octrain)

models <- list("log transformed linear regression model" = log_reg_model, 
               "log transformed multiple regression model" = log_multi_reg_model, 
               "multiple regression model" = multi_reg_model)

for(i in 1:3){
  x <- summary(models[[i]])$r.squared
  y <- names(models[i])
  
  print(sprintf("%s r.squared: %g", y, x))
}
```

And so it looks like our log transformed model using all predictors has the highest r-squared. This is also a signficant improvement over the simple OLS regression model we fit earlier. Now let's put it to the ultimate test by making predictions on the test set.

```{r}
#create model predictions, purrr:map()
model_predictions <- as.data.frame(map(models, predict, octest))

#revert the log transformed predictions back to original scale
model_predictions <- model_predictions %>%
  transmute(`multiple regression model` = multiple.regression.model,
            `log transformed linear regression model` = exp(log.transformed.linear.regression.model),
         `log transformed multiple regression model` = exp(log.transformed.multiple.regression.model))

#add predictions to octest data set
octest_pred <- octest %>%
  bind_cols(model_predictions)

#gather the predicted columns
octest_pred <- octest_pred %>%
  gather(key = model, value = predictions, 7:9)

#plot the results
ggplot(octest_pred, aes(x = predictions, y  = income, color = model))+
  geom_point() +
  geom_abline()+
  geom_smooth(method = "glm", se = F)
  #facet_grid(~type)


```

The black line indicates a perfect fit where the predicted and actual values of income are equal. Of course, this is not the case. If we look at the slopes for each model, we see that there skewed upwards toward the income axis meaning that the predicted values are estimated lower than the true outcome. The log transformed multiple regression model is behaves similarly than the other models for the bulk of the data and has the bet predicted values, most closely associated to the true income value for that observation than the other models. It's not by much, but at least it's something.

We can examine the $R^2$ for the models based on the test data.

```{r}
`multiple regression r.squared` <- R2(pred = model_predictions$`multiple regression model`, obs = octest$income)
`log transformed linear regression r.squared` <- R2(pred = model_predictions$`log transformed linear regression model`,
                                                    obs = octest$income)
`log transformed multiple reression r.squared` <- R2(pred = model_predictions$`log transformed multiple regression model`,
                                                     obs = octest$income)

rsquared_values <- c("OLS test R^2" = olstest_r.squared, "Multiple Regression Test R^2" = `multiple regression r.squared`, "Log Transformed Linear Regression Test R^2" = `log transformed linear regression r.squared`, "Log Transformed Multiple Regression Test R^2" = `log transformed multiple reression r.squared`) 

for(i in 1:length(rsquared_values)){
  x <- names(rsquared_values)[i]
  y <- rsquared_values[i]
  
  print(sprintf("%s: %g", x, y))
  }
```

The log transformed multiple regression model holds up on the test set! 
