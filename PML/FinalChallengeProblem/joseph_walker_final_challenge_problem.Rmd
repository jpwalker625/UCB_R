---
title: "Final Challenge Problem"
author: "Joseph Walker"
date: "3/28/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = T, warning = F, message = F, eval = T, cache = T)
```

# Problem 1

**Use the dataset birth_data.csv to build a model that determines whether a baby is at risk, i.e., needs immediate emergency care or extra medical attention immediately upon birth.**
  
*Requirements*
  
* Use caret for your solution, see:https://topepo.github.io/caret/  
* Use caret-based data preparation to prepare the data for your models.
* Use caret to fit (1) logistic regression and (2) GBM models.
* Use caret to tune your models appropriately.
* Use caret (and other methods, if desired) to evaluate and compare the performance of each type of model using appropriate methods (e.g., confusion matrix, ROC, AUC).

Let's begin by loading the dataset.

```{r}
#load required packages
library(tidyverse)
library(caret)
library(ggplot2)

#read in dataset
birth_data <- read_csv(file = "birth_data.csv")

#examine the dataset
glimpse(birth_data)
```

There are two variables with a character data type. Let's take a look at these variables more closely to see how many distinct observations there are for each.

```{r}
birth_data %>%
  select(GESTREC3, DPLURAL) %>%
  distinct()
```

Because there are only two and three categories for each variable respectively, it would better suit us to convert these variables to factors.

```{r}
birth_data <- within(birth_data, {
  GESTREC3 <- factor(GESTREC3)
  DPLURAL <- factor(DPLURAL)
})

#convert var names to lower case
names(birth_data) <- tolower(names(birth_data))
```

And one more thing we can do to make the dependent variable clearer for modeling purposes is to change the FALSE/TRUE to No/Yes. This will help down the road when we want to convert our classes into class probabilities for comparing models using performance metrics such AUC.

```{r}
#convert T/F to Yes/No
birth_data$atrisk <- ifelse(birth_data$atrisk == TRUE, "Yes", "No")

#convert atrisk to factor
birth_data$atrisk <- factor(birth_data$atrisk)
```

Now that we've cleaned up the variables, let's examine the dataset at a high level.

```{r}
summary(birth_data)
```

It looks like we're working with a clean data set; no missing values (NA's) or obvious discrepancies.

As is good modeling practice, first we begin by splitting our data set into a training and test set.

```{r}
#set seed for reproducibility
set.seed(1849)

#create training index vector
train_index <- createDataPartition(y = birth_data$atrisk, p = .8, list = F)

#create training set
birth_train <- birth_data[train_index, ]

#create test set
birth_test <- birth_data[-train_index, ]
```

## Logistic Regression

We'll start by building a logistic regression model.

```{r}
#create train control object
glm_control <- trainControl(method = "cv", number = 10)

#build the model
glm_model <- train(atrisk ~ ., method = "glm", data = birth_train, family= "binomial", trControl = glm_control, trace = F)

#examine the model
glm_model
```

Using 10 fold cross-validation, we were able to build a model with an accuracy of `r paste(round(glm_model$results$Accuracy * 100, 2), '%')`

Now let's apply the model to the test set.

```{r}
#make predictions on test set using glm model
glm_predictions <- predict(glm_model, newdata = birth_test)

#create Confusion Matrix
caret::confusionMatrix(glm_predictions, reference = birth_test$atrisk, positive = "Yes")
```

Again, the model results in a high accuracy of ~98% on the test set. Looking at the confusion matrix, one can gather that the model has a high specificity meaning that it does an excellent job at predicting cases where babies are not at risk at birth (True Negatives). On the other hand, the model has a very low sensitivity and predicts only 1 of the 96 total cases of babies being at risk correctly. This is obviously not a good thing as this could have life and death consequences for the cases of babies who truly are at risk but were not classified as such by the model.

## Gradient Boosting Machines

Let's move on to some different models to see how they stack up against the logistic regression model (GLM). In this section, we'll continue using the `caret` package to make 2 **Gradient Boosting Machine** models. 

We'll start with a basic **Gradient Boosted Regression - GBM** model. The first step is to create a train control object to specify that we want a 10-fold cross-validation repeated 10 times. 

```{r}
#set seed for reproducibility
set.seed(5964)

#create train control for gbm
gbm_control <- trainControl(method = "repeatedcv", number = 10, repeats = 10, verboseIter = F)

#build gbm model
gbm_model <- train(atrisk ~ ., data = birth_train, method = "gbm", trControl = gbm_control, verbose = F)

#visualize the model
ggplot(gbm_model) +
  labs(title = "GBM Model Results", subtitle = "10 Fold CV, Repeated 10 Times")

#examine the best tuning parameters
gbm_model$bestTune
```

The best tuning parameters achieve a model accuracy of `r paste(round(max(gbm_model$results$Accuracy) * 100, 2), '%')`

Once again, let's apply this model to the test set.
```{r}
#make predictions on test set using gbm model
gbm_predictions <- predict(gbm_model, birth_test)

#make confusion matrix to compare predicted vs. actual
caret::confusionMatrix(data = gbm_predictions, reference = birth_test$atrisk, positive = "Yes")
```

It looks like we were able to improve the **sensitivity** using GBM! The number of at risk births the model was able to identify (True Positives) increased from 1/96 in the glm model to 12/96. It's a good start but it's still not good enough! 

The model we built above was rather basic in that we didn't tune any of the custom parameters available to us. Let's explore some of these options to see if we can improve upon the model.

First, we need to see what tuning parameters are available with the GBM model:

```{r}
#lookup the gbm model
modelLookup("gbm")
```

The relevant tuning parameters we'll want to customize here are **n.trees** - how many trees or ensembles the model will iterate over - and **interaction.depth** - the max depth, or number of branches each tree will develop to find the best classification criteria. 

First we'll build a train control specifiying the number of folds for the validation process. Then we'll create a custom tuning grid specifying the various parameters we want to test. 

*In this next step, I am also going to change the outcomes from prediction classes ("yes/no") to prediction probabilities so that we can later use the ROC/AUC parameters to compare the various models.*

```{r}
#set seed for reproducibility
set.seed(952)

#create 10 fold CV train control
custom_gbm_control <- trainControl(method = "cv", number = 10, classProbs = T, summaryFunction = twoClassSummary, verboseIter = F)

#define tuning parameter values
n.trees <- (2:10) * 50
depth <- c(2, 6, 10, 14, 18)
shrinkage <- 0.1 #default value
minobs <- 10

#create fully crossed grid of tuning parameters
gbm_grid <- expand.grid(n.trees = n.trees, interaction.depth = depth, shrinkage = shrinkage, n.minobsinnode = minobs)

#build custom gbm model
custom_gbm_model <- train(atrisk ~ ., data = birth_train, method = "gbm", trControl = custom_gbm_control, tuneGrid = gbm_grid, verbose = F)

#visualize the gbm results
ggplot(custom_gbm_model)

#examine the optimal model parameters
custom_gbm_model$bestTune
```

Whew! That took some time. Of course that's what we'd expect. The time it takes to compute the models increases depending on the size of the data set and the size of the tuning grid. Time is definitely a major factor to consider when training models. The expense of time may not be a proper trade off if the model improvements are small and not as beneficial to you. In our case, you can't put a price on life, and the difference between a few percentage or even tenths of percentage points can make a signficant difference.

As indicated above, we can see the optimal tuning parameters lead to an ROC value of `r round(max(custom_gbm_model$results$ROC), 3)`

Let's follow through with this model to see how the predicitons on the test set turn out.
```{r}
#make predictions
custom_gbm_predictions <- predict(custom_gbm_model, newdata = birth_test)

#make confusion matrix to test predictions vs. actuals
caret::confusionMatrix(data = custom_gbm_predictions, reference = birth_test$atrisk, positive = "Yes")
```

It looks like we're actually one True Positive observation worse than the previous model. That's definitely not what we want. 

As I mentioned before, using the predicted class probabiities opened up some options for us. I am going to search the model to find the parameters that lead to the highest sensitivity.

```{r}
#find the row with the highest sensitivty
max_sens <- which.max(custom_gbm_model$results$Sens)

#get optimal parameters for max sensitivity
max_sens_gbm_params <- custom_gbm_model$results[max_sens, ]

#examine the parameters which give the model with the highest sensitivity
max_sens_gbm_params
```

And we can visualise this to make sure it matches up with the parameters above.

```{r}
ggplot(custom_gbm_model, metric = "Sens")
```

Now, let's plug these parameters into our model and see how it performs on the test set.

```{r}
#set seed for reproducibility
set.seed(4711)

#create train control for the model
max_sens_control <- trainControl(method = "none", verboseIter = F)
#create a data frame with the optimized values
max_sens_grid <- data.frame(n.trees = 200, interaction.depth = 2, n.minobsinnode = 10, shrinkage = 0.1)

#build the model
max_sens_gbm <- train(atrisk ~ ., data = birth_train, method = "gbm", trControl = max_sens_control, tuneGrid = max_sens_grid, verbose = F)

#make predictions on the test set
max_sens_gbm_predictions <- predict(max_sens_gbm, newdata = birth_test)

#make confusion matrix
caret::confusionMatrix(max_sens_gbm_predictions, reference = birth_test$atrisk, positive = "Yes")
```

After all that work, the model sensitivity improved by one True Positive prediction. At this point, if we wanted to make improvements, we'd consider using another model or finding better predictors. To give you an idea of the importance of the predictors, or lack thereof:

```{r}
library(gbm)
var_importance <- varImp(max_sens_gbm)

ggplot(var_importance)
```

Most of the predictors have very little, if no importance to the model! Another problem with this data is that the 'atrisk' response variable classes are extremely imbalanced. 

In this assignment, I chose to use the confusion matrix to compare the performance of the models as I believe that it best captures the importance of the repercussions of sensitivity vs. specificity for this dataset. We can also use AUC and ROC to measure model performance.

```{r}
library(ModelMetrics) #for caclculating AUC

actuals <- birth_test$atrisk

#calculate aucs for each model
logit_auc <- auc(predicted =  glm_predictions,  actual = actuals)
gbm_auc <- auc(predicted = gbm_predictions, actual = actuals)
tuned_gbm_auc <- auc(predicted = max_sens_gbm_predictions, actual = actuals)
```

```{r, echo=FALSE}
sprintf("logistic regression model test AUC: %.3f", logit_auc)
sprintf("Generalized Boosting Model test AUC: %.3f", gbm_auc)
sprintf("Custom tuned GBM for Max Sensitivity model test AUC: %.3f", tuned_gbm_auc)
```

Finally, we can attain similar results with a visual perspective:

```{r}
library(ROCR) #for visualizing ROC curves
# List of predictions converted to binary values
preds_list <- list(ifelse(glm_predictions == "Yes", 1, 0), ifelse(gbm_predictions == "Yes", 1, 0), ifelse(max_sens_gbm_predictions == "Yes", 1, 0))

# List of actual values (same for all)
m <- length(preds_list)

actuals_list <- rep(list(ifelse(birth_test$atrisk == "Yes", 1, 0)), m)

# Plot the ROC curves
pred <- prediction(predictions = preds_list, labels = actuals_list)
rocs <- performance(prediction.obj = pred, measure = "tpr", x.measure = "fpr")
plot(rocs, col = as.list(1:m), main = "Test Set ROC Curves")
legend(x = "bottomright", 
       legend = c("Logit Reg. Model", "GBM Model", "Custom GBM Model"),
       fill = 1:m)
```

# Problem 2

**Use the dataset ocdata.csv having the following fields - education, income, women, prestige, census, type - to answer the questions below.**

**(Problem 2A) Fit a univariate OLSR (Ordinary Least Squares Regression) model, adhering to OLSR assumptions, predicting income from prestige only.**
  
**(Problem 2B) Fit a model of any type we discussed in class, using all meaningful predictors of income, to obtain the "best" results, using whatever method you wish. Use of caret for Problem 2 may be helpful, but is not required. Be sure to properly prepare your data, take steps to avoid overfitting, tune and evaluate the performance of your model, and provide a clear description and analysis of your results. Caution: for your solution, more is not better. Use only an appropriate model and methods that provides value. Avoid unnecessary work. Also, pay attention to the appearance and quality of your HTML product: attractive, crisp, and clear.**

First we'll begin by importing the data set.

```{r}
#read in the dataset
ocdata <- read_csv("ocdata.csv")

#examine the dataset
glimpse(ocdata)
```

The only variable that is not numeric is `type`. Let's take a closer look at this variable to see if we need to convert it to a categorical variable (factor).

```{r}
#examine the distinct values in the type variable
ocdata %>%
  select(type) %>%
  distinct()

#make the type variable a factor
ocdata$type <- factor(ocdata$type)
```

Now let's summarize the data to look for anything that might stick out immediately.

```{r}
summary(ocdata)
```

There are four NA's in the *type* variable. We'll have to filter these out so that we don't have any problems with out modeling later on.

```{r}
#omit rows with NAs
ocdata <- na.omit(ocdata)

#Check to see if there are any NA values
any(is.na(ocdata))
```


## Problem 2A
Our first task is to build an ordinary least squares regression model predicting income using only the prestige variable. Before we begin, let's first visualize the relationship between the two variables to make sure there's a linear relationship.

```{r}
ggplot(ocdata, aes(x = prestige, y = income)) +
  geom_point() +
  geom_smooth(method = "lm")
```

This is interesting. While the relationship between the two variables is linear *for the most part*, it appears that there is something happening on the right tail which results in significantly larger income values at the upper end of the prestige.

We can check to see whether or not there are any trends in the residuals after we fit the model. But first, let's split the data into a training and test set. Since our data set is rather small, I'm going to use a 70:30 train/test split rather than the traditional 80/20.

```{r}
set.seed(6436)

nrows <- nrow(ocdata)

index <- sample.int(n = length(ocdata$income), size = nrows * 0.7, replace = F)

octrain <- ocdata[index, ]
octest <- ocdata[-index, ]
```

Now let's build the model.
```{r}
set.seed(478)
library(modelr)
library(broom)

#create linear model
ols <- lm(formula = income ~ prestige, data = octrain)

#augment the dataset using the ols model
ols_augmented <- augment(ols)

#plot the residuals
ggplot(data= ols_augmented, aes(x = .fitted, y = .resid)) +
  geom_point()

#determine the mean of the resids
mean(ols_augmented$.resid)
```

The good news is that the residuals *practically* have a mean of 0. This checks off one of the necessary assumptions required of OLS regression. 

Let's see how the model performed over all. The RMSE tells us how much error is associated with each predicted value, smaller RMSE is always better. The $R^2$ tells us how well the fit of the model is. In other words, how well does the predictor variable, in this case *prestige*, explain the response variable, *income*. A value of 1 is ideal meaning that we have a perfect fit, where as 0 means we'd be just as good at guessing the values than trying to use the model to make predictions.

```{r}
sprintf("RMSE: %g", rmse(ols, octrain))
sprintf("R-squared: %g", rsquare(ols, octrain))
```

Now, let's take a look at how the model performs on the test set.

```{r}
#add predictions to the octest set
olstest_pred <- add_predictions(data = octest, model = ols, var = "predicted")

#plot actual vs. predicted
ggplot()+
  geom_point(data = olstest_pred, aes(x = predicted, y = income, color = "test data")) +
  geom_point(data= ols_augmented, aes(x = .fitted, y = income, color = "train data")) +
  scale_colour_manual(name = "Values", values = c("test data" = "red3", "train data" = "steelblue4"))
```

Both the training data and test data indicate that the model signficantly under estimates the predicted income value at the upper end, i.e. when the prestige reaches the highest levels. Let's examine this trend from another perspective.

```{r}
#plot the data
ggplot(olstest_pred, aes(x = prestige)) +
  geom_point(aes(y = income, color = "actual")) +
  geom_point(aes(y = predicted, color = "predicted")) +
  scale_color_manual(name = "Values", values = c("actual" = "red3", "predicted" = "steelblue4"))

#calculate rsquared and rmse for test set
olstest_rmse <- RMSE(pred = olstest_pred$predicted, obs = olstest_pred$income)
olstest_r.squared <- R2(pred = olstest_pred$predicted, obs = olstest_pred$income)
```

## Problem 2B

The model above uses only one predictor variable to generate an ordinary least squares regression model. What happens if we take a look at some of the other variables the data has to offer. And also why not try a different modeling approach. Let's start by taking a look at a scatterplot matrix of the data to see if we gain any insight.

```{r}
pairs(ocdata)
```

It appears that education shares a similar relationship to income as prestige does. To better assess this relationship we can take a look at the correlation values to see which is stronger.

```{r}
#relationship between education and income
cor(ocdata$education, ocdata$income)

#relationship between prestige and income
cor(ocdata$prestige, ocdata$income)
```

Prestige actually has the strong correlation. If we were to use both of these predictors in the dataset, we may run into the issue of overfitting the data due to co-correlation.

One interesting observation is that the income plotted agains prestige and education has a *hockey stick* trend. That is, the income values dramatically increase at the high end of the other variables producing a plot that looks like a hockey stick.

This is a frequent occurrence when dealing with monetary values. We can fix this by using a log transformation on the income response variable. Let's examine the distribution of the income data to get a better understanding of this issue.
```{r}
ggplot(ocdata, aes(x = income)) +
  geom_density()
```

We can see that there is a long right tail indicating that the income data isn't normally distributed. Most of the data is centered around 0 to 10,000 range, with a few outliers beyond that. Let's see what happens when we log transform the income.

```{r}
ggplot(ocdata, aes(x = log(income))) +
  geom_density()
```

Now the income data is normally distributed. Now let's see how we can apply this transformation to the modeling procedure.

Let's build three models, two of which take the log transformation of the response variable income. The first model will use only prestige as the predictor, the second will include the entire dataset because we still don't know whether these other variables will provide us any additional predictive power. The third data set will not use log transformation on income and will use all available predictors to fit the model. We can think of this third model as our control to see whether taking the log of the response is better or not.

```{r}
#create formulas
log_reg_formula <- as.formula("log(income) ~ prestige")
log_multireg_formula <- as.formula("log(income) ~ .")
multi_reg_formula <- as.formula("income ~ .")

#build models
log_reg_model <- lm(log_reg_formula, octrain)
log_multi_reg_model <- lm(log_multireg_formula, octrain)
multi_reg_model <- lm(multi_reg_formula, octrain)

models <- list("log transformed linear regression model" = log_reg_model, 
               "log transformed multiple regression model" = log_multi_reg_model, 
               "multiple regression model" = multi_reg_model)

for(i in 1:3){
  x <- summary(models[[i]])$r.squared
  y <- names(models[i])
  
  print(sprintf("%s r.squared: %g", y, x))
}
```

And so it looks like our log transformed model using all predictors has the highest r-squared. This is also a signficant improvement over the simple OLS regression model we fit earlier. Now let's put it to the ultimate test by making predictions on the test set.

```{r}
#create model predictions, purrr:map()
model_predictions <- as.data.frame(map(models, predict, octest))

#revert the log transformed predictions back to original scale
model_predictions <- model_predictions %>%
  transmute(`multiple regression model` = multiple.regression.model,
            `log transformed linear regression model` = exp(log.transformed.linear.regression.model),
         `log transformed multiple regression model` = exp(log.transformed.multiple.regression.model))

#add predictions to octest data set
octest_pred <- octest %>%
  bind_cols(model_predictions)

#gather the predicted columns
octest_pred <- octest_pred %>%
  gather(key = model, value = predictions, 7:9)

#plot the results
ggplot(octest_pred, aes(x = predictions, y  = income, color = model))+
  geom_point() +
  geom_abline()+
  geom_smooth(method = "glm", se = F)


```

The black line indicates a perfect fit where the predicted and actual values of income are equal. Of course, this is not the case. If we look at the slopes for each model, we see that each model is skewed upwards toward the y axis meaning that the predicted values are estimated lower than the true outcome. For any data point below the black line, the predicted value is estimated to be less than the income value, where as any point above the black line indicates a value higher than the true income value. The log transformed multiple regression model behaves similarly to the other models on the bulk of the data set. It shines at the problematic outliers; the predicted values have a little less residual error than the observations associated with the other models.

We can examine the $R^2$ for the models based on the test data.

```{r}
`multiple regression r.squared` <- R2(pred = model_predictions$`multiple regression model`, obs = octest$income)
`log transformed linear regression r.squared` <- R2(pred = model_predictions$`log transformed linear regression model`,
                                                    obs = octest$income)
`log transformed multiple reression r.squared` <- R2(pred = model_predictions$`log transformed multiple regression model`,
                                                     obs = octest$income)

rsquared_values <- c("OLS test R^2" = olstest_r.squared, "Multiple Regression Test R^2" = `multiple regression r.squared`, "Log Transformed Linear Regression Test R^2" = `log transformed linear regression r.squared`, "Log Transformed Multiple Regression Test R^2" = `log transformed multiple reression r.squared`) 

for(i in 1:length(rsquared_values)){
  x <- names(rsquared_values)[i]
  y <- rsquared_values[i]
  
  print(sprintf("%s: %g", x, y))
  }
```

The log transformed multiple regression model holds up on the test set!