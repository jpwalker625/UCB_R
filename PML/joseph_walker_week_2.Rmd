---
title: "Assignment 2 - Part 2"
author: "Joseph Walker"
date: `r sys.Date()`
output: html_document
---



```{r, echo= TRUE}
#Define Global Variables
knitr::opts_chunk$set(echo = T, warning = F, message = F,eval = T)

#load required packages
library(tidyverse)
library(broom)
library(modelr)
library(ggpmisc)
```

1) One downside of the linear model is that it is sensitive to unusual values because the distance 
incorporates a squared term. Fit a linear model to the simulated data below, and visualise the 
results.

```{r}
#set seed for reproducibility
set.seed(123)

# Define simulated dataset
sim1a <- tibble(
  x = rep(1:10, each = 3),
  y = x * 1.5 + 6 + rt(length(x), df = 2)
)

#build linear model and tidy it
model <- lm(y~x, data = sim1a) %>% tidy()

#view the model stats
tidy_model

#assign slope coefficient of x
slope <- round(model$estimate[2], 3)

ggplot(sim1a, aes(x = x, y = y))+
  geom_point() +
  geom_smooth(method = "lm", se = F) +
  annotate(geom = "text", x = 4, y = 20, label = paste("slope = ", slope, sep =" "))
```

Rerun a few times to generate different simulated datasets. What do you notice about the
model?

Variation on the x and y axis has different effects on the model. For instance, variation in the response (y) can increase the variation in the slope. In contrast, the greater the variance in the explanatory variable (x) leads to less variance of the slope coefficient. It is also important to note that the greatest shift in the model occurs when the variance in the response data (y)  is greater at the extreme ends of the explanatory variable. This is because the extreme ends of the explanatory variable (x) act as anchor points for the model. 
```{r}
# set seed for reproducibility
set.seed(2755)

#create empty data frame
sim <- tibble()

#loop over 3 iterations of the simulated dataset and save to dataframe
for(i in seq(3)) {
  temp <- tibble (
  x = rep(1:10, each = 3),
  y = x * 1.5 + 6 + rt(length(x), df = 2),
  rep = i)
  
  sim <- rbind(temp, sim)
}

#nest the data frame
sim_by_rep <- sim %>% group_by(rep) %>% nest(-rep)

#create model function
model <- function(data) {
  lm(y ~ x, data = data)
}

#apply model and tidy function to each nested group
tidy_models <- sim_by_rep %>%
  mutate(model = map(data, model)) %>%
  mutate(model = map(model, tidy)) %>%
  arrange(rep)

#view the model summaries
tidy_models$model

#assign slopes of models 
slopes <-  tidy_models %>%
   select(-data) %>% 
   unnest() %>%
   filter(term == "x") %>%
  pull(estimate) %>%
  round(2)
   

#visualize the models
ggplot(sim, aes(x = x, y = y, color = factor(rep)))+
  geom_point() +
  stat_smooth(method = 'lm', se= F) +
  guides(color = guide_legend(title = "replicate")) +
  annotate(geom = "text", x = 3, y = 22,
           label = paste("Slopes",
                         "\n", 
                         "1 (red) = ", slopes[1], 
                         "\n", 
                         "2 (green)= ", slopes[2],
                         "\n",
                         "3 (blue)= ", slopes[3],
                         sep = ""))
```


2) One way to make linear models more robust is to use a different distance measure. For example, instead of root-mean-squared distance, you could use mean-absolute distance:

> measure_distance <- function(mod, data) {
    diff <- data$y - make_prediction(mod, data)
    mean(abs(diff))
    }

Use optim() to fit this model to the simulated data above and compare it to the linear model.

```{r}

model1 <- function(a, data) {
  a[1] + data$x * a[2]
}

measure_distance <- function(mod, data) {
  diff <- data$y - model1(mod, data)
  mean(abs(diff))
}

optimized_params <- optim(c(0, 0), measure_distance, data= sim1a)

optimized_params <- optimized_params$par

#compare optim model to original model
ggplot(sim1a, aes(x=x, y = y)) +
  geom_point() +
  geom_smooth(method = "lm", se = F) +
  geom_abline(slope = optimized_params[2], intercept = optimized_params[1], linetype = 'dashed', color = "red")
```

