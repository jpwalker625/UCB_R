---
title: "Assignment for week 8"
author: "Joseph Walker"
date: "March 6, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = TRUE, warning = FALSE, message = FALSE)
```

**Use the banking telemarketing dataset bank-additional-full.csv, (in bank-additional-full.zip posted on the Files/Assignment for Week 8) to build a model that determines whether a customer will subscribe to a bank term deposit (outcome variable "y"). Use logistic regression, decision trees (with hyperparameter optimization), and random forest models to generate your results. Evaluate and compare the performance of each type of model using appropriate tools and methods (e.g., confusion matrix, ROC/AUC).**

#Step 1: Import Data

```{r}
#load tidyverse for data wrangling
library(tidyverse)

#read in the data file
bank_data <- read.csv("bank-additional-full.csv", sep = ";", header = TRUE)

#examine the data
summary(bank_data)
```

# Step 2: Tidy, Wrangle, Transform

Initially summarizing the data allows us to get a high level overview. We can see what type of variables we're dealing with (nominal or categorical), whether or not there's any missing data (NA's or NULL's), and how we may need to alter the data to work with it more efficiently.

Fortunately, this data set is rather clean but there are a few modifications to make. I personally find periods (.) annoying in column names so let's change those.

```{r}
#for manipulating strings
library(stringr)

names(bank_data) <- str_replace_all(string = names(bank_data), pattern = '\\.', replacement = "_")
```

# Step 3: Modeling

Before we begin to build any models, we first need to split the data. We will split the data into a training set and a test set. The training set is used to build the model while the test set is used the evaluate the model performance by comparing the predicted outcomes to the actual outcomes.

*Be careful not to train and test a model with the same dataset. This is known as overfitting and may cause the model to be too specific to the particular dataset you are using. Since the purpose of building a predictive model is to make accurate predictions on events that have not yet occurred (the future), we need a model that isn't too specifc to the data set it was built upon. This is a term known as overfitting.  Using a train and test set split is one way of doing this.*

It is standard practice to partition the data using an 80/20 split: 80% of the data is used for training and 20% is used for testing.
```{r}
set.seed(127)

#get number of rows in the data set
nrows <- nrow(bank_data)

#create sample index for splitting
sample <- sample.int(nrows, size = .8 * nrows, replace = FALSE)

#split data into train and test set
bank_train <- bank_data[sample, ]
bank_test <- bank_data[-sample, ]

paste("number of rows in full data set:", nrow(bank_data))
paste("Number of rows in training set: ", nrow(bank_train))
paste("Number of rows in test set: ", nrow(bank_test))
```

## Logistic Regression

To start, let's build a model using **logistic regression**.

First, we need a formula.
```{r}
fmla <- as.formula("y ~ .")
```

Our response variable in this case is `y` and the rest of the variables of the dataset are predictors, denoted by `.`

Now let's build a logistic regression model using the `glm` function
```{r}
glm_model <- glm(formula = fmla, data = bank_train, family = binomial(link = "logit"))
```

To examine the model, we can use the `glance` function from the `broom` package.
```{r}
library(broom)

#examine the model terms
glance(glm_model)
```

Now, let's proceed with making predictions on the test set.

```{r}
glm_preds <- predict(object = glm_model,
                 newdata = bank_test, 
                 type = "response")
```

Using the `prediction` function, the predicted probabilities of the response variable are computed. In our case, we have predicted the probability of a customer subscribing to a bank term deposit. In order to convert probabilities to actual class predictions, a threshold has to be set. 

```{r}
#convert probabiities to classifications
glm_pred_class <- ifelse(glm_preds > 0.5, "yes", "no")

#Examine the error rate with a Confusion Matrix
library(caret)

glm_matrix <- confusionMatrix(data = glm_pred_class, reference = bank_test$y, positive = "yes")

glm_matrix
```

It is useful to know that you can specify the positive predicted class with the `positive` argument. Otherwise R defaults th positive value to the first level/factor.  
  
The accuracy of the model is `r glm_matrix$overall['Accuracy']`.  Pretty good!
  
Another useful statistic is the `no-information rate` which tells us how well our model would be at predicting the class probabilities purely by guessing. If the no-information rate is greater than the accuracy of the model, it's time to start looking for some better predictors!

```{r}
#Is the Accuracy better than the NIR?
glm_matrix$overall["Accuracy"] > glm_matrix$overall["AccuracyNull"]

#And the p-value that ACC > NIR
glm_matrix$overall["AccuracyPValue"]
```

This threshold can be modified to change the sensitivity and specificity of the model depending on what your purpose is. For example, if we decrease the probability threshold, we allow more observation to be classified as a True Positive value, therefore increasing the sensitivity of the model. This comes at the cost of decreasing the specificity of the model since the false positive rate increases.

```{r}
#predictions with higher sensitivity
sensitive <- ifelse(glm_preds > .1, "yes", "no")

sensitive_matrix <-confusionMatrix(data = sensitive, reference = bank_test$y, positive = "yes")

sensitive_matrix$table
```

In contrast, if we increase the probability threshold, we decrease the amount of observations classified as True Positives but we increase the certainty of these observations being right. This also improves the model's ability to correctly identify true negatives. In this case, the model's sensitivity is low, but the specificity is high. 

```{r}
#predictions with higher specificity
specific <- ifelse(glm_preds > .9, "yes", "no")

specific_matrix <- confusionMatrix(specific, bank_test$y, positive = "yes")

specific_matrix$table
```

## Decision Tree

In this section, we'll model the data using a decision tree. Tree based models are useful for making decisions or numeric predictions and have three main advantages: *they are easy to use, easy to interpret, and usually accurate.* We'll also look at various hyper parameters which are like knobs we can tweak to tune the model. 

To make a recursive partioning decision tree, we'll use the `rpart` package.

```{r}
library(rpart) #for modelling decision trees

#model data using rpart
tree_model <- rpart(formula = y ~ .,
                    data = bank_train,
                    method = "class",
                    parms = list(split = 'gini'))



tree_preds <- predict(object = tree_model,
                  newdata = bank_test,
                  type = 'class')
```

One of the hyper parameters of a decision tree we can adjust is the splitting criterion. With `rpart`, the default splitting criterion is `gini`, which is a measurement of the impurity of the partitioning. Because the purpose of a decision tree is to partition the data into groups as homogeneously as possible, it makes sense to measure the amount of impurity.  The code above was set to gini for the purpose of showing how to adjust the splitting criterion.  
  
Now, let's use the `information` splitting criterion which seeks to define the degree of disorganization in the dataset, also known as Entropy. The information split is based on the theory that less impure groups require less information to describe them. In other words, they do not have as much complexity and therefore are less disorganized.

```{r}
tree_model2 <- rpart(formula = y ~ .,
                    data = bank_train,
                    method = "class",
                    parms = list(split = 'information'))

tree_preds2 <- predict(object = tree_model2,
                  newdata = bank_test,
                  type = 'class')
```

We can compare the models by calculating the classification error with the `ce` function from the `ModelMetrics` package. The classification error can be calculated as:  $$ false\ positives\ +\ false\ negatives \over total\ observations$$

```{r}
library(ModelMetrics) # for calculating classification error

#compare the classification error of the models
paste("gini split model : ", ce(actual = bank_test$y, predicted = tree_preds))
paste("information split model: ", ce(actual = bank_test$y, predicted = tree_preds2))
```

Both models perform similarly and have a classification error rate of just under 9%.  

The classification error is also $$ 1- accuracy $$ which can be calcualted as: $$ true\ positives\ +\ false\ negatives \over total\ observations$$

Rather than having to do the math each time, we can obtain the accuracy from the confusion matrix function.

Now, let's examine the models in more detail by looking at the accuracy and confusion matrices for each.

*Be careful when calling a confusion matrix because both the caret and Model Metrics packages have a confusion matrix function. While they behave similarly, the names of the arguments differ slightly and may throw you off depending which function is being called.*

```{r}
#confusion marix with gini split model
gini_matrix <- caret::confusionMatrix(data = tree_preds, reference = bank_test$y, positive = "yes")

#confusion matrix with information split model
info_matrix <- caret::confusionMatrix(data = tree_preds2, reference = bank_test$y, positive = "yes")

gini_matrix$overall["Accuracy"]
gini_matrix$table

info_matrix$overall["Accuracy"]
info_matrix$table
```

The model using the information split does a better job at predicting customers who will subscribe to a bank term deposit. We can say that this model has a higher sensitivity since it has a higher True Positive rate. However, this comes at the cost of having a higher false positive rate. This explains why this model has a slightly lower accuracy that the model using the gini split. 

From a business standpoint, the best model depends on what you need. If the cost to be wrong is too high, you'd want a model with a higher specificity; the one with a lower false positive rate. On the other hand, if you can tolerate error, the model that identifies more true positives has a higher sensitivity and may be the right one for you.

And finally, we can finish this section by visualizing the classification trees using the `rpart.plot` package.

```{r}
library(rpart.plot) #for visualizing decision trees

#visualize the gini split model classification tree
rpart.plot(tree_model, type = 3, main= "Decision tree using the 'gini' split")

#visualize the information split model classification tree
rpart.plot(tree_model2, type = 3, main= "Decision tree using the 'information split")
```

## Other hyper parameters

The other primary tuning parameters used to adjust decision trees are **minsplit, maxdepth,** and **complexity probability**.

```{r}
hyperparamter <- rpart(y ~ ., data = bank_test, method = "class", control = rpart.control(minsplit = 10, maxdepth = 5))

rpart.plot(hyperparamter)

hyp_preds <- predict(object = hyperparamter,
                     newdata = bank_test,
                    type = "class")

hyperparamter$cptable
plotcp(hyperparamter)
caret::confusionMatrix(data = hyp_preds, reference = bank_test$y )
```


---
## Random Forest Model

The last type of model we'll explore is the **random forest**. Random forest modeling employs a method known as ensemble modeling in which many classification trees are aggregated to find the best one. At each split of the tree, only a random subset of variables are used in order to reduce the correlation betwen sampled trees and hopefully, improve the performance of the model overall.

Each tree in a random forest model is created using a bootstrapped sample of the dataset used to train the model. In order to compare all the trees of the model, random forests compute the **out of bag error rate** for each tree by testing the data that was not included in the bootstrapped sample (this is know as out of bag data) and comparing it to the true classification vlaues to calculate the classification error.

To make a random forest model, we'll use the `randomForest` package but be aware that you can also use the `ranger` package. 

As a reminder, each tree in a random forest is made using a random bootstrapped sample so we'll use the `set.seed` function for reproducibility.
```{r}
#load the random Forest package
library(randomForest)
#set the seed for reproducibility
set.seed(3918)

rf_model <- randomForest(formula = y ~ ., data = bank_train)

#examine the random forest model
rf_model
```

Right out of the box, or rather, out of the bag I should say, the random forest model estimates an out of bag error rate of 8.4%. The confusion matrix indicates that the model is more specific than it is sensitive. 

Like the models we trained in earlier sections, random forest also have hyper parameters we can tune:  
  
* ntree: number of trees (default is set at 500)
* mtry: number of variables randomly sampled as candidates at each split
* sampsize: number of samples to train on
* nodesize: minimum size (number of samples) of the terminal nodes
* maxnodes: maximum number of terminal nodes

Before we dive in, let's visualize the random forest model witha call to the `plot` function

```{r}
#plot the random forest model
plot(rf_model)

# Add a legend
legend(x = "right", 
       legend = colnames(rf_model$err.rate),
       fill = 1:ncol(rf_model$err.rate))
```

As the plot shows, the model reaches a minimum error rate at around 100 trees so it doesn't really make sense for us to tweak the **ntree** parameter.

As for the **mtry** hyper parameter, the `randomForest` package has a specific function for optimizing the mtry value and that is `tuneRF()`. The function starts with the default mtry value and iterates stepwise by some specified value in the left and right direction to find the mtry value that leads to the lowest OOB error estimate. 

```{r}
#set the seed for reproducibility
set.seed(57)

#optimize the mtry hyper paramter with tuneRF
tuneRF(x = select(bank_train, -y),
                     y = bank_train$y, 
                     ntreeTry = 500, 
                     plot = T, #defaults to True 
                     doBest = T #defaults to False
)

```

The **sampsize** is a bit tricky. By default, the `randomForest` model algorithm uses a bootstrap sample apporach meaning the sample size is equal to the size of the dataset used to train the model. Furthermore, it uses bootstrap sampling with replacement to generate the randomness of the sampling for each tree. In general, this default method is sufficient but we can customize it. Of course, there are drawbacks. If we use a smaller sample size, we increase the variation in each tree, but we may sacrifice performance of the model. On the other hand, increasing the sample size will lead to less randomness and may lead to overfitting.

Depending on the size of your data or whether you have specific classification criteria you have in mind, it may or may not makes sense to tune the **nodesize** and **maxnodes**. 

Rather than spending countless hours manually optimizing these tuning parameters, we can set up a hyper grid where we specify a range of values that we can create many models on and find the best model. Let's get to it.

```{r}
# Establish a list of possible values for mtry, nodesize and sampsize
mtry <- seq(from = 4, to = ncol(bank_train) * 0.8, by = 3)
maxnodes <- seq(from = 4, to = 12, by = 2)
sampsize <- nrow(bank_train) * c(0.5, 1, 1.5)

# Create a data frame containing all combinations 
hyper_grid <- expand.grid(mtry = mtry, maxnodes = maxnodes, sampsize = sampsize)

# Create an empty vector to store OOB error values
oob_err <- c()

# Write a loop over the rows of hyper_grid to train the grid of models
for (i in 1:nrow(hyper_grid)) {

    # Train a Random Forest model
    model <- randomForest(formula = default ~ ., 
                          data = bank_train,
                          mtry = hyper_grid$mtry[i],
                          maxnodes = hyper_grid$maxnodes[i],
                          sampsize = hyper_grid$sampsize[i],
                          replace = T #imperative if sample size is larger than # observations in datastet
                          )
                          
    # Store OOB error for the model                      
    oob_err[i] <- model$err.rate[nrow(model$err.rate), "OOB"]
}

# Identify optimal set of hyperparmeters based on OOB error
opt_i <- which.min(oob_err)
print(hyper_grid[opt_i,])
```


It's also important to keep in mind the goals/costs of the problem you're working on. A model with 90% accuracy rate might be good enough for some. But in our case, the difference between predicting 90% accuracy vs. 91% accuracy the amount of customers who will subscribe to a bank term deposit may cost millions of dollars for the company and is worth the small yet crucial gain in model performance.




---
When working with classification models, it is often easier to compare the performance of different types of models with a standard metric such as AUC and to do so we need the response variable to be a numeric value (binary 1, 0) rather than classification (yes, no). 

Let's create a new variable "subscribe" that equals 1 when y is "yes", and 0 when y is "No". 
```{r}
bank_data <- bank_data %>%
  mutate(subscribe = ifelse(y =='yes', 1, 0))
```