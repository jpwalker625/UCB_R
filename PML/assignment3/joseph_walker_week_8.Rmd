---
title: "Assignment for week 8"
author: "Joseph Walker"
date: "March 6, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = TRUE, warning = FALSE, message = FALSE)
```

**Use the banking telemarketing dataset bank-additional-full.csv, (in bank-additional-full.zip posted on the Files/Assignment for Week 8) to build a model that determines whether a customer will subscribe to a bank term deposit (outcome variable "y"). Use logistic regression, decision trees (with hyperparameter optimization), and random forest models to generate your results. Evaluate and compare the performance of each type of model using appropriate tools and methods (e.g., confusion matrix, ROC/AUC).**

#Step 1: Import Data

```{r}
#load tidyverse for data wrangling
library(tidyverse)

#read in the data file
bank_data <- read.csv("bank-additional-full.csv", sep = ";", header = TRUE)

#examine the data
summary(bank_data)
```

# Step 2: Tidy, Wrangle, Transform

Initially summarizing the data allows us to get a high level overview. We can see what type of variables we're dealing with (nominal or categorical), whether or not there's any missing data (NA's or NULL's), and how we may need to alter the data to work with it more efficiently.

Fortunately, this data set is rather clean but there are a few modifications to make. I personally find periods (.) annoying in column names so let's change those.

```{r}
#for manipulating strings
library(stringr)

names(bank_data) <- str_replace_all(string = names(bank_data), pattern = '\\.', replacement = "_")
```

# Step 3: Modeling

Before we begin to build any models, we first need to split the data. We will split the data into a training set and a test set. The training set is used to build the model while the test set is used the evaluate the model performance by comparing the predicted outcomes to the actual outcomes.

*Be careful not to train and test a model with the same dataset. This is known as overfitting and may cause the model to be too specific to the particular dataset you are using. Since the purpose of building a predictive model is to make accurate predictions on events that have not yet occurred (the future), we need a model that performs well in general, not just on past performance. Using a train and test set split is one way of doing this.*

It is standard practice to partition the data using an 80/20 split: 80% of the data is used for training and 20% is used for testing.
```{r}
#get number of rows in the data set
nrows <- nrow(bank_data)

#create sample index for splitting
sample <- sample.int(nrows, size = .8 * nrows, replace = FALSE)

#split data into train and test set
bank_train <- bank_data[sample, ]
bank_test <- bank_data[-sample, ]

paste("number of rows in full data set:", nrow(bank_data))
paste("Number of rows in training set: ", nrow(bank_train))
paste("Number of rows in test set: ", nrow(bank_test))
```

## Logistic Regression

To start, let's build a model using **logistic regression**.

First, we need a formula.
```{r}
fmla <- as.formula("y ~ .")
```

Our response variable in this case is `y` and the rest of the variables of the dataset are predictors, denoted by `.`

Now let's build a logistic regression model using the `glm` function
```{r}
glm_model <- glm(formula = fmla, data = bank_train, family = binomial(link = "logit"))
```

To examine the model, we can use the `glance` function from the `broom` package.
```{r}
library(broom)

glance(glm_model)
```

Now, let's proceed with making predictions on the test set.

```{r}
glm_preds <- predict(object = glm_model,
                 newdata = bank_test, 
                 type = "response")
```

Using the `prediction` function, the predicted probabilities of the response variable are computed. In our case, we have predicted the probability of a customer subscribing to a bank term deposit. In order to convert probabilities to actual class predictions, a threshold has to be set. 

```{r}
#convert probabiities to classifications
glm_pred_class <- ifelse(glm_preds > 0.5, "yes", "no")

#Examine the error rate with a Confusion Matrix
library(caret)

glm_matrix <- confusionMatrix(data = glm_pred_class, reference = bank_test$y, positive = "yes")

glm_matrix
```

It is useful to know that you can specify the positive predicted class with the `positive` argument. Otherwise R defaults th positive value to the first level/factor.  
  
The accuracy of the model is `r glm_matrix$overall['Accuracy']`.  Pretty good!
  
Another useful statistic is the `no-information rate` which tells us how well our model would be at predicting the class probabilities purely by guessing. If the no-information rate is greater than the accuracy of the model, it's time to start looking for some better predictors!

```{r}
#Is the Accuracy better than the NIR?
glm_matrix$overall["Accuracy"] > glm_matrix$overall["AccuracyNull"]

#And the p-value that ACC > NIR
glm_matrix$overall["AccuracyPValue"]
```

This threshold can be modified to change the sensitivity and specificity of the model depending on what your purpose is. For example, if we decrease the probability threshold, we allow more observation to be classified as a True Positive value, therefore increasing the sensitivity of the model. This comes at the cost of decreasing the specificity of the model since the false positive rate increases.

```{r}
#predictions with higher sensitivity
sensitive <- ifelse(glm_preds > .1, "yes", "no")

sensitive_matrix <-confusionMatrix(data = sensitive, reference = bank_test$y, positive = "yes")

sensitive_matrix$table
```

In converse, if we increase the probability threshold, we decrease the amount of observations classified as True Positives but we increase the certainty of these observations being right. This also improves the model's ability to correctly identify true ngatives.In this case, the model's sensitivity is low, but the specificity is high. 

```{r}
#predictions with higher specificity
specific <- ifelse(glm_preds > .9, "yes", "no")

specific_matrix <- confusionMatrix(specific, bank_test$y, positive = "yes")

specific_matrix$table
```

## Decision Tree

In this section, we'll model the data using a decision tree and look at different hyper parameters to tune the model. Tree based models are useful for making decisions or numeric predictions and have three main advantages: they are easy to use, easy to interpret, and often accurate.

To make a recursive partioning decision tree, we'll use the `rpart` package.

```{r}
library(rpart) #for modelling decision trees

#model data using rpart
tree_model <- rpart(formula = y ~ .,
                    data = bank_train,
                    method = "class",
                    parms = list(split = "gini"))



tree_preds<- predict(object = tree_model,
                  newdata = bank_test,
                  type = 'class')
```

One of the hyper parameters of a decision tree we can adjust is the splitting criterion. With `rpart`, the default splitting criterion is `gini`, which is a measurement of the impurity of the partitioning. Because the purpose of a decision tree is to partition the data into groups as homogeneously as possible, it makes sense to measure the amount of impurity.  The code above was set to gini for the purpose of showing how to adjust the splitting criterion. Now, let's use the `information` splitting criterion which seeks to define the degree of disorganization in the dataset, also known as Entropy. The information split is based on the theory that less impure groups require less information to describe them. In other words, they do not have as much complexity and therefore are less disorganized.

```{r}
tree_model2 <- rpart(formula = y ~ .,
                    data = bank_train,
                    method = "class",
                    parms = list(split = "information"))

tree_preds2 <- predict(object = tree_model2,
                  newdata = bank_test,
                  type = 'class')
```

We can compare the models by calculating the classification error with the `ce` function from the `ModelMetrics` package. The classification error is also $$ 1- accuracy $$ of which we can obtain from the confusion matrix.

```{r}
library(ModelMetrics) # for calculating classification error

#compare the classification error of the models
paste("gini split model : ", ce(actual = bank_test$y, predicted = preds1))
paste("information split model: ", ce(actual = bank_test$y, predicted = preds2))
```

Both models perform similarly and have a classification error rate of about 9%.

Let's examine the models in more detail with a confusion matrix.

*Be careful when calling a confusion matrix because both the caret and Model Metrics packages have a confusion matrix function. While they behave similarly, the names of the arguments differ slightly and may throw you off depending which function is being called.*

```{r}
#confusion marix with gini split model
gini_matrix <- caret::confusionMatrix(data = tree_preds, reference = bank_test$y, positive = "yes")

#confusion matrix with information split model
info_matrix <- caret::confusionMatrix(data = tree_preds2, reference = bank_test$y, positive = "yes")

gini_matrix$overall["Accuracy"]
gini_matrix$table

info_matrix$overall["Accuracy"]
info_matrix$table
```

And finally, we can visualize the classification trees using the `rpart.plot` package.

```{r}
library(rpart.plot) #for visualizing decision trees

#visualize the gini split model classification tree
rpart.plot(tree_model, type = 3, main= "Factors influencing whether or not \n a customer will subscribe to a bank term deposit")

#visualize the information split model classification tree
rpart.plot(tree_model2, type = 3, main= "Factors influencing whether or not \n a customer will subscribe to a bank term deposit")
```


Another hyper paramter we can tune is the

```{r}
hyperparamter <- rpart(y ~ ., data = bank_test, method = "class", control = rpart.control(minsplit = 1, maxdepth = 5))

rpart.plot(hyperparamter)

hyp_preds <- predict(object = hyperparamter,
                     newdata = bank_test,
                    type = "class")

caret::confusionMatrix(data = hyp_preds, reference = bank_test$y)
```

```{r}

models <- list()
preds <- list()

for(i in 1:length(splits)){
  splits <- c("gini", "information")
  
  model <- rpart(formula = y ~ ., 
                   data = bank_test, 
                   method = "class",
                   parms = list(split = splits[i])
                   )
 
 predictions <- predict(object = model,
                  newdata = bank_test,
                  type = "class")
 
 models[[i]] <- model
 preds[[i]] <- predictions
}

caret::confusionMatrix(data = preds[[1]], reference = bank_test$y)$table
caret::confusionMatrix(data = preds[[2]], reference = bank_test$y)$table
```

---
## Random Forest Model


When working with classification models, it is often easier to compare the performance of different types of models with a standard metric such as AUC and to do so we need the response variable to be a numeric value (binary 1, 0) rather than classification (yes, no). 

Let's create a new variable "subscribe" that equals 1 when y is "yes", and 0 when y is "No". 
```{r}
bank_data <- bank_data %>%
  mutate(subscribe = ifelse(y =='yes', 1, 0))
```